---
title: "Prediction of academic achievement in adolescents from teacher reports of inattention in childhood - a methodological pattern classification study"
output: html_notebook
---

```{r, echo=TRUE, eval=FALSE}
~GitHub/inattention-population/code/inattention-populationsample-cart.Rmd
```

<small>
This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 
Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 
Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.
When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file).
</small>


## Abstract

### Background
Inattentive behavior is associated with academic problems. The present study investigates primary school teacher reports on nine items reflecting different aspects of inattention, with an aim to reveal patterns of behavior predicting high-school academic achievement. To that end, we used different types of pattern analysis and machine learning methods. 

### Methods
Inattention in a sample 2397 individuals were rated by their primary school teachers when they participated in the first wave of the Bergen Child Study (BCS) (7 - 9 years old), and their academic achievements were available from an official school register when attending high-school (16 - 19 years old). Inattention was assessed by the nine items rated at a categorical leve, and the academic achievement scores were divided into three parts including a similar number of participants. 

### Results 
Boys obtained higher inattention scores and lower academic scores than girls. Inattention problems related to sustained attention and distractibility turned out to have the highest predictive value of academic achievement level across all selected statistical analyses, and the full model showed that inattention explained about 10\% of the variance in high school scores about 10 years later. A high odds-ration of being allocated to the lowest academic achievement category was shown by a multinominal regression analysis, while a pattern of problems related to sustained attention and distractibility was revealed by generating classification trees. By including recursive learning algorithms, the most successful classification was found between these inattention items and the highest level of achievement scores. 

### Summary 
The present study showed the importance of a pattern of early problems related to sustained attention and distractibility in predicting future academic results. By including different statistical classification models we showed that this pattern was fairly consistent. Furthermore, calculation of classification errors gave information about the uncertainty when predicting the outcome for individual children. Further studies should include a wider range of variables. 




<small>Organization of the data and the analysis:</small>

Libraries being used:

* memisc - spss.system.file()
* psych  - headTail(), describe()
* Hmisc - describe()
* pander - pander(), panderOptions()

<img src="../images/Data_to_classes_20160205_pptx.jpg" width="500px" height="500px" />

### Data preparation

Input file:

 * inattention_Arvid_new.sav (from Astri, on ~/Dropbox/Arvid_inatteion/data2)
 * inattention_nomiss_2397x12.csv
 
Output files (data):

 * inattention_nomiss_2397x12_snap_is_0_1_2.csv
 * inattention_nomiss_2397x12_snap_is_0_1.csv
 * inattention_nomiss_2397x12_snap_is_0_1_2_outcome_is_L_M_H.csv (Low, Medium, High academic score)
 * inattention_nomiss_2397x12_snap_is_0_1_2_outcome_is_0_1_2.csv (all numerical)
 * inattention_nomiss_2397x12_snap_is_N_S_C_outcome_is_L_M_H.csv (Not, Somewhat, Certainly true)
 

```{r, echo=TRUE, eval=TRUE} 
fn <- "../data2/inattention_Arvid_new.sav"
```

```{r, echo=TRUE, eval=FALSE}
# The original SPSS file as provided to AJL is
# 'inattention_Astri_94_96_new_grades_updated.sav'
# and being edited and reduced by AJL to 'inattention_Arvid_new.sav'
# Import data stored in the SPSS format
library(memisc)
fn <- "../data2/inattention_Arvid_new.sav"
data <- as.data.set(spss.system.file(fn))

# Make new data frame from the sample with the variables 
# gender, grade, SNAP1, ..., SNAP9 (vars #1-11) and
# academic_achievement (var #52) 
names(data)
d <- data[, c(1:11, 52)]
dim(d)
names(d)
str(d)
summary(d)
```

```{r, echo=TRUE, eval=TRUE}
D3 <- read.csv(file = "../data/inattention_nomiss_2397x12_snap_is_0_1_2.csv") 
C <- read.csv(file = "../data/inattention_nomiss_2397x12_snap_is_0_1_2_outcome_is_L_M_H.csv")
D <- read.csv(file = "../data/inattention_nomiss_2397x12_snap_is_N_S_C_outcome_is_L_M_H.csv")
E <- read.csv(file = "../data/inattention_nomiss_2397x12_snap_is_0_1_2_outcome_is_0_1_2.csv")
str(D3)
head(D3)
str(C)
head(C)
str(D)
head(D)
str(E)
head(E)
```

```{r, echo=TRUE, eval=TRUE}
# Dataset for classification 
library(Hmisc)
describe(C)
describe(D)
#library(pastecs)
#tab <- stat.desc(C)
#knitr::kable(tab, digits = 2, caption = 'Summary C')
```


## CART

From https://www.r-bloggers.com/a-brief-tour-of-the-trees-and-forests:

Tree methods such as CART (classification and regression trees) can be used as alternatives to logistic regression. It is a way that can be used to show the probability of being in any hierarchical group. The following is a compilation of many of the key R packages that cover trees and forests.  The goal here is to simply give some brief examples on a few approaches on growing trees and, in particular, the visualization of the trees. These packages include classification and regression trees, graphing and visualization, ensemble learning using random forests, as well as evolutionary learning trees. There are a wide array of package in R that handle decision trees including trees for longitudinal studies.  I have found that when using several combinations of these packages simultaneously that some of the function begin to fail to work.



## Conditional Inference Tree for SNAP inattention data (categorical predictor variables)


### party:

*party*: A Laboratory for Recursive Partytioning

A computational toolbox for recursive partitioning. The core of the package is ctree(), an implementation of conditional inference trees which embed tree-structured regression models into a well defined theory of conditional inference procedures. This non-parametric class of regression trees is applicable to all kinds of regression problems, including nominal, ordinal, numeric, censored as well as multivariate response variables and arbitrary measurement scales of the covariates. Based on conditional inference trees, cforest() provides an implementation of Breiman's random forests. The function mob() implements an algorithm for recursive partitioning based on parametric models (e.g. linear models, GLMs or survival regression) employing parameter instability tests for split selection. Extensible functionality for visualizing tree-structured regression models is available.


```{r, echo=TRUE, eval=TRUE}
library(party)
# Classification
frmla = averBinned ~ gender + grade + snap1 + snap2 + snap3 + 
    snap4 + snap5 + snap6 + snap7 + snap8 + snap9
fit.cla <- ctree(frmla, data=D)
print(fit.cla)
plot(fit.cla)
class(fit.cla)  # class type 
#Table of prediction errors
table(predict(fit.cla), D$averBinned)
# Estimated class probabilities
tr.pred = predict(fit.cla, newdata=D, type="prob")
print(head(tr.pred))
```

```{r, echo=TRUE, eval=TRUE}
n = nrow(D)
txt = sprintf("Conditional Inference Tree (ctree) for categorical SNAP inattention variables (n=%d)", n) 
plot(fit.cla, main=txt)
```

```{r, echo=TRUE, eval=TRUE}
# The class predictions of the tree for the learning sample (resubstitution)
summary(D$averBinned)
summary(predict(fit.cla))
``` 


### partykit:

The partykit package provides a flexible toolkit with infrastructure for learning, representing, summarizing, and visualizing a wide range of tree-structured regression and classification models. The functionality encompasses: (a) Basic infrastructure for repre- senting trees (inferred by any algorithm) so that unified print/plot/predict methods are available. (b) Dedicated methods for trees with constant fits in the leaves (or terminal nodes) along with suitable coercion functions to create such tree models (e.g., by rpart, RWeka, PMML). (c) A reimplementation of conditional inference trees (ctree, originally provided in the party package). (d) An extended reimplementation of model-based recur- sive partitioning (mob, also originally in party) along with dedicated methods for trees with parametric models in the leaves. This vignette gives a brief overview of the package and discusses in detail the generic infrastructure for representing trees (a). Items (b)â€“(d) are discussed in the remaining vignettes in the package.

Function partykit::ctree is a reimplementation of (most of) party::ctree employing the new party infrastructure of the partykit infrastructure. Although the new code was already extensively tested, it is not yet as mature as the old code. If you notice differences in the structure/predictions of the resulting trees, please contact the package maintainers. See also vignette("ctree", package = "partykit") for some remarks about the internals of the different implementations.

Conditional inference trees estimate a regression relationship by binary recursive partitioning in a conditional inference framework. Roughly, the algorithm works as follows: 1) Test the global null hypothesis of independence between any of the input variables and the response (which may be multivariate as well). Stop if this hypothesis cannot be rejected. Otherwise select the input variable with strongest association to the response. This association is measured by a p-value corresponding to a test for the partial null hypothesis of a single input variable and the response. 2) Implement a binary split in the selected input variable. 3) Recursively repeate steps 1) and 2).

The implementation utilizes a unified framework for conditional inference, or permutation tests, developed by Strasser and Weber (1999). The stop criterion in step 1) is either based on multiplicity adjusted p-values (testtype = "Bonferroni" in ctree_control) or on the univariate p-values (testtype = "Univariate"). In both cases, the criterion is maximized, i.e., 1 - p-value is used. A split is implemented when the criterion exceeds the value given by mincriterion as specified in ctree_control. For example, when mincriterion = 0.95, the p-value must be smaller than $0.05$ in order to split this node. This statistical approach ensures that the right-sized tree is grown without additional (post-)pruning or cross-validation. The level of mincriterion can either be specified to be appropriate for the size of the data set (and 0.95 is typically appropriate for small to moderately-sized data sets) or could potentially be treated like a hyperparameter (see Section~3.4 in Hothorn, Hornik and Zeileis, 2006). The selection of the input variable to split in is based on the univariate p-values avoiding a variable selection bias towards input variables with many possible cutpoints. The test statistics in each of the nodes can be extracted with the sctest method. (Note that the generic is in the strucchange package so this either needs to be loaded or sctest.constparty has to be called directly.) In cases where splitting stops due to the sample size (e.g., minsplit or minbucket etc.), the test results may be empty.

Predictions can be computed using predict, which returns predicted means, predicted classes or median predicted survival times and more information about the conditional distribution of the response, i.e., class probabilities or predicted Kaplan-Meier curves. For observations with zero weights, predictions are computed from the fitted tree when newdata = NULL.

By default, the scores for each ordinal factor x are 1:length(x), this may be changed for variables in the formula using scores = list(x = c(1, 5, 6)), for example.

For a general description of the methodology see Hothorn, Hornik and Zeileis (2006) and Hothorn, Hornik, van de Wiel and Zeileis (2006).

Hothorn T, Hornik K, Zeileis A (2006). Unbiased Recursive Partitioning: A Conditional Inference Framework. Journal of Computational and Graphical Statistics, 15(3), 651â€“674.

Hothorn T, Zeileis A (2015). partykit: A Modular Toolkit for Recursive Partytioning in R. Journal of Machine Learning Research, 16, 3905â€“3909.
http://www.jmlr.org/papers/v16/hothorn15a.html

*party* - A class for representing decision trees and corresponding accessor functions.

Objects of class party basically consist of a partynode object representing the tree structure in a recursive way and data. The data argument takes a data.frame which, however, might have zero columns. Optionally, a data.frame with at least one variable (fitted) containing the terminal node numbers of data used for fitting the tree may be specified along with a terms object or any additional (currently unstructured) information as info. Argument names defines names for all nodes in node.

Method names can be used to extract or alter names for nodes. Function node_party returns the node element of a party object. Further methods for party objects are documented in party-methods and party-predict. Trees of various flavors can be coerced to party, see party-coercion.

Two classes inherit from class party and impose additional assumptions on the structure of this object: Class constparty requires that the fitted slot contains a partitioning of the learning sample as a factor ("fitted") and the response values of all observations in the learning sample as ("response"). This structure is most flexible and allows for graphical display of the response values in terminal nodes as well as for computing predictions based on arbitrary summary statistics.

Class simpleparty assumes that certain pre-computed information about the distribution of the response variable is contained in the info slot nodes. At the moment, no formal class is used to describe this information.

*partynode* - A class for representing inner and terminal nodes in trees and functions for data partitioning.

A node represents both inner and terminal nodes in a tree structure. Each node has a unique identifier id. A node consisting only of such an identifier (and possibly additional information in info) is a terminal node.

Inner nodes consist of a primary split (an object of class partysplit) and at least two kids (daughter nodes). Kid nodes are objects of class partynode itself, so the tree structure is defined recursively. In addition, a list of partysplit objects offering surrogate splits can be supplied. Like partysplit objects, partynode objects aren't connected to the actual data.

Function kidids_node() determines how the observations in data[obs,] are partitioned into the kid nodes and returns the number of the list element in list kids each observations belongs to (and not it's identifier). This is done by evaluating split (and possibly all surrogate splits) on data using kidids_split.

Function fitted_node() performs all splits recursively and returns the identifier id of the terminal node each observation in data[obs,] belongs to. Arguments vmatch, obs and perm are passed to kidids_split.

Function formatinfo_node() extracts the the info from node and formats it to a character vector using the following strategy: If is.null(info), the default is returned. Otherwise, FUN is applied for formatting. The default function uses as.character for atomic objects and applies capture.output to print(info) for other objects. Optionally, a prefix can be added to the computed character string.

All other functions are accessor functions for extracting information from objects of class partynode.


```{r, echo=TRUE, eval=TRUE}
library(partykit) 
txt = sprintf("Conditional Inference Tree (ctree D in partykit) for categorical SNAP inattention variables (n=%d)", n) 
frmla = averBinned ~ gender + grade + snap1 + snap2 + snap3 + 
    snap4 + snap5 + snap6 + snap7 + snap8 + snap9
fit.pcla <- ctree(frmla, data=D)
class(fit.pcla)  # different class from before
print(fit.pcla)
#Table of prediction errors
table(predict(fit.pcla), D$averBinned)
# Estimated class probabilities
tr.pred = predict(fit.pcla, newdata=D, type="prob")
print(head(tr.pred))
str(fit.pcla[2])
```

```{r, echo=TRUE, eval=TRUE}
plot(fit.pcla, gp = gpar(fontsize = 8),     # font size changed to 6, or 8
  inner_panel=node_inner,
  ip_args=list(
       abbreviate = FALSE, 
       id = TRUE),
  main=txt
  )
```

```{r, echo=TRUE, eval=TRUE}
pdf("../manuscript/Figs/cart_partykit_ctree_D.pdf", width = 8, height = 5)
plot(fit.pcla, gp = gpar(fontsize = 8),     # font size changed to 6, or 8
  inner_panel=node_inner,
  ip_args=list(
       abbreviate = FALSE, 
       id = TRUE),
  main=txt
  )
dev.off()
```


```{r, echo=TRUE, eval=TRUE}
library(partykit) 
txt = sprintf("Conditional Inference Tree (ctree C in partykit) for categorical SNAP inattention variables (n=%d)", n) 
frmla = averBinned ~ gender + grade + snap1 + snap2 + snap3 + 
    snap4 + snap5 + snap6 + snap7 + snap8 + snap9
fit.Cpcla <- ctree(frmla, data=C)
class(fit.Cpcla)  # different class from before
print(fit.Cpcla)
#Table of prediction errors
table(predict(fit.Cpcla), D$averBinned)
# Estimated class probabilities
tr.Cpred = predict(fit.Cpcla, newdata=C, type="prob")
print(head(tr.Cpred))
```

```{r, echo=TRUE, eval=TRUE}
plot(fit.Cpcla, gp = gpar(fontsize = 8),     # font size changed to 6, or 8
  inner_panel=node_inner,
  ip_args=list(
       abbreviate = FALSE, 
       id = TRUE),
  main=txt
  )
```

```{r, echo=TRUE, eval=TRUE}
pdf("../manuscript/Figs/cart_partykit_ctree_C.pdf", width = 8, height = 5)
plot(fit.Cpcla, gp = gpar(fontsize = 8),     # font size changed to 6, or 8
  inner_panel=node_inner,
  ip_args=list(
       abbreviate = FALSE, 
       id = TRUE),
  main=txt
  )
dev.off()
```

```{r, echo=TRUE, eval=TRUE}
library(partykit) 
txt = sprintf("Conditional Inference Tree (ctree E in partykit) for categorical SNAP inattention variables (n=%d)", n) 
E$averBinnedF = as.factor(E$averBinned)
E <- subset(E, select = -c(averBinned))
frmla = averBinnedF ~ gender + grade + snap1 + snap2 + snap3 + 
    snap4 + snap5 + snap6 + snap7 + snap8 + snap9
fit.Epcla <- ctree(frmla, data=E)
class(fit.Epcla)  # different class from before
print(fit.Epcla)
#Table of prediction errors
table(predict(fit.Epcla), E$averBinnedF)
# Estimated class probabilities
tr.Epred = predict(fit.Epcla, newdata=E, type="prob")
print(head(tr.Epred))
```

```{r, echo=TRUE, eval=TRUE}
plot(fit.Epcla, gp = gpar(fontsize = 8),     # font size changed to 6, or 8
  inner_panel=node_inner,
  ip_args=list(
       abbreviate = FALSE, 
       id = TRUE),
  main=txt
  )
```
```{r, echo=TRUE, eval=TRUE}
pdf("../manuscript/Figs/cart_partykit_ctree_E.pdf", width = 8, height = 5)
plot(fit.Epcla, gp = gpar(fontsize = 8),     # font size changed to 6, or 8
  inner_panel=node_inner,
  ip_args=list(
       abbreviate = FALSE, 
       id = TRUE),
  main=txt
  )
dev.off()
```


#### rpart:

This package includes several example sets of data that can be used for recursive partitioning and regression trees.  Categorical or continuous variables can be used depending on whether one wants classification trees or regression trees. This package as well at the tree package are probably the two go-to packages for trees.  However, care should be taken as the tree package and the rpart package can produce very different results.

```{r, echo=TRUE, eval=TRUE}
library(rpart)

frmla = averBinned ~ gender + grade + snap1 + snap2 + snap3 + 
    snap4 + snap5 + snap6 + snap7 + snap8 + snap9
fit = rpart(frmla, method="class", data=D)
printcp(fit) # display the results
plotcp(fit) # visualize cross-validation results
summary(fit) # detailed summary of splits
# plot tree
plot(fit, uniform=TRUE, 
     main=list("Classification Tree for SNAP, Gender, and Grade\n", 
               cex = 1.0, col = "black", font = 3))
text(fit, use.n=TRUE, all=TRUE, cex=.6)
# tabulate some of the data
table(subset(D, grade=='3rd')$averBinned)
```



#### tree:

This is the primary R package for classification and regression trees.  It has functions to prune the tree as well as general plotting functions and the mis-classifications (total loss). The output from tree can be easier to compare to the General Linear Model (GLM) and General Additive Model (GAM) alternatives.

```{r, echo=TRUE, eval=TRUE}
library(tree)
tr = tree(averBinned ~ ., data=D)
summary(tr)
plot(tr)
text(tr)
```

#### maptree:

Graph a classification or regression tree with a hierarchical tree diagram, optionally including colored symbols at leaves and additional info at intermediate nodes.

```{r, echo=TRUE, eval=TRUE}
library(maptree)
library(cluster)
library(rpart)
draw.tree(clip.rpart(rpart(D), best=7), nodeinfo=TRUE, units="",cases="obs", digits=0)
```



<!-- regular html comment 


--> 



