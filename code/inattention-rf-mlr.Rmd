---
title: "Prediction of academic achievement in adolescents from teacher reports of inattention in childhood - a pattern classification study" 
subtitle: "AJ Lundervold, T Bø, A Lundervold"
output: html_notebook
---

```{r, echo=TRUE, eval=FALSE}
~GitHub/inattention-population/code/inattention-rf-mlr.Rmd
```

<small>
This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 
Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 
Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.
When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file).
</small>


<small>Organization of the data and the analysis:</small>

<img src="../images/Data_to_classes_pptx.jpg" width="500px" height="500px" />

### Data preparation

Builds on processing in *~GitHub/inattention-population/code/inattention-data-prep.Rmd*

```{r, echo=TRUE, eval=FALSE}
# In ~GitHub/inattention-population/code/inattention-populationsample-data-prep.Rmd we set:
# D$ave <- as.numeric(D$ave)
# D$snap1 <- mapvalues(as.factor(D$snap1), from = c("Not true","Somewhat true","Certainly true"), to = c("0","1","2"))
# # ...
# # Dataset for classification based on D3 and discretized average academic achievemnt
# C <- D3
# C$averBinned <- cut(aver, cutpoints, right=FALSE, include.lowest=TRUE,
#                      labels=c("L","M","H"))
# C <- subset(C, select = -c(ave))
# write.csv(C, file = "../data/inattention_nomiss_2397x12_snap_is_0_1_2_outcome_is_L_M_H.csv",row.names=FALSE)
```

```{r, echo=TRUE, eval=TRUE}
D3 <- read.csv(file = "../data/inattention_nomiss_2397x12_snap_is_0_1_2.csv") 
C <- read.csv(file = "../data/inattention_nomiss_2397x12_snap_is_0_1_2_outcome_is_L_M_H.csv")
D <- read.csv(file = "../data/inattention_nomiss_2397x12_snap_is_N_S_C_outcome_is_L_M_H.csv")
E <- read.csv(file = "../data/inattention_nomiss_2397x12_snap_is_0_1_2_outcome_is_0_1_2.csv")
str(C)
head(C)
```

```{r, echo=TRUE, eval=TRUE}
# Dataset for classification 
#library(Hmisc)
#describe(C)
library(pastecs)
tab <- stat.desc(C)
knitr::kable(tab, digits = 2, caption = 'Summary C')
```

### Random Forest for SNAP inattention data 

Random forests are very good in that it is an ensemble learning method used for classification and regression.  It uses multiple models for better performance that just using a single tree model.  In addition because many sample are selected in the process a measure of variable importance can be obtain and this approach can be used for model selection and can be particularly useful when forward/backward stepwise selection is not appropriate and when working with an extremely high number of candidate variables that need to be reduced.

    
```{r, echo=TRUE, eval=TRUE}
library(randomForest)
C$averBinnedC = as.factor(C$averBinned)  # To perform clasification and not regression
C <- subset(C, select = -c(averBinned))
frmla = averBinnedC ~ gender + grade + snap1 + snap2 + snap3 + 
    snap4 + snap5 + snap6 + snap7 + snap8 + snap9
fit.Crf = randomForest(frmla, data=C)
print(fit.Crf)
plot(fit.Crf)
```

#### Extract variable importance measure:

Here are the definitions of the variable importance measures. The first measure is computed from permuting OOB data: For each tree, the prediction error on the out-of-bag portion of the data is recorded (error rate for classification, MSE for regression). Then the same is done after permuting each predictor variable. The difference between the two are then averaged over all trees, and normalized by the standard deviation of the differences. If the standard deviation of the differences is equal to 0 for a variable, the division is not done (but the average is almost always equal to 0 in that case).

The second measure is the total decrease in node impurities from splitting on the variable, averaged over all trees. For classification, the node impurity is measured by the Gini index. For regression, it is measured by residual sum of squares.

The function being plotted in Partial dependence plot *partialPlot* is defined as:

$$ \tilde{f}(x) = \frac{1}{n} ∑_{i=1}^n f(x, x_{iC}), $$

where $x$ is the variable for which partial dependence is sought, and $x_{iC}$ is the other variables in the data. The summand is the predicted regression function for regression, and logits (i.e., log of fraction of votes) for which.class for classification:

$$ f(x) = \log p_k(x) - \frac{1}{K} ∑_{j=1}^K \log p_j(x), $$
where $K$ is the number of classes, $k$ is which.class, and $p_j$ is the proportion of votes for class $j$.





```{r, echo=TRUE, eval=TRUE}
importance(fit.Crf)
plot(importance(fit.Crf), lty=2, pch=16)
lines(importance(fit.Crf))
varImpPlot(fit.Crf)
```


#### CORElearn:

This is a great package that contain many different machine learning algorithms and functions.  It include trees, forests, naive Bayes, locally weighted regression, among others.

Builds a classification or regression model from the data and formula with given parameters. Classification models available are

 * random forests, possibly with local weighing of basic models (parallel execution on several cores),

 * decision tree with constructive induction in the inner nodes and/or models in the leaves,

* kNN and weighted kNN with Gaussian kernel,

 * naive Bayesian classifier.

Regression models:

 * regression trees with constructive induction in the inner nodes and/or models in the leaves,

 * linear models with pruning techniques,

 * locally weighted regression,

 * kNN and weighted kNN with Gaussian kernel.




```{r, echo=TRUE, eval=TRUE}
library(CORElearn)
## Random Forests
frmla = averBinnedC ~ gender + grade + snap1 + snap2 + snap3 + 
    snap4 + snap5 + snap6 + snap7 + snap8 + snap9
fit.C.rand.forest = CoreModel(frmla, data=C, model="rf", selectionEstimator="MDL", minNodeWeightRF=5, rfNoTrees=500)
print(fit.C.rand.forest)
plot(fit.C.rand.forest)
```



## Cross-validating an RF model


### mlr: Machine Learning in R
https://www.rdocumentation.org/packages/mlr/versions/2.9?

```{r, echo=TRUE, eval=TRUE}
# http://stackoverflow.com/questions/36802846/how-to-tune-multiple-parameters-using-caret-package
# http://mlr-org.github.io/mlr-tutorial/release/html/measures/index.html

library(mlr)
library(psych)

# Read and check the data
C <- read.csv(file = "../data/inattention_nomiss_2397x12_snap_is_0_1_2_outcome_is_L_M_H.csv")
str(C)
headTail(C)

# Create a classification, regression, survival, cluster, cost-sensitive classification or multilabel task
# https://rdrr.io/cran/mlr/man/Task.html
# id - Id string for object. Default is the name of the R variable passed to data.
# data - A data frame containing the features and target variable(s).
# target - Name(s) of the target variable(s).
rpart.task = classif.task = makeClassifTask(id = "inattention", data = C, target = "averBinned")


# Create a description object for a resampling strategy
# https://rdrr.io/cran/mlr/man/makeResampleDesc.html
# method - “CV” for cross-validation, “LOO” for leave-one-out, “RepCV” for repeated cross-validation, 
# “Bootstrap” for out-of-bag bootstrap, “Subsample” for subsampling, “Holdout” for holdout.
# predict -  What to predict during resampling: “train”, “test” or “both” sets. Default is “test”.
# iters - Number of iterations, for “CV”, “Subsample” and “Boostrap”.
# reps - Repeats for “RepCV”. Here iters = folds * reps. Default is 10.
# folds - Folds in the repeated CV for RepCV. Here iters = folds * reps. Default is 10.
# stratify - Should stratification be done for the target variable? For classification tasks, 
#   this means that the resampling strategy is applied to all classes individually and the resulting 
#   index sets are joined to make sure that the proportion of observations in each training 
#   set is as in the original data set. Useful for imbalanced class sizes.
# We will use 10-fold cross-validation with 10 repetitions to assess the quality of a 
# specific parameter setting. 
resamp = makeResampleDesc("RepCV", predic = "test", folds = 10, reps = 10, stratify = TRUE)

# Create learner object
# https://rdrr.io/cran/mlr/man/makeLearner.html
# cl - Class of learner. By convention, all classification learners start with “classif.”, 
#   all regression learners with “regr.”, all survival learners start with “surv.”, 
#   all clustering learners with “cluster.”, and all multilabel classification learners start with
#   “multilabel.”. A list of all integrated learners is available on the learners help page.
# predict.type - Classification: “response” (= labels) or “prob” (= probabilities and labels by 
#   selecting the ones with maximal probability). Regression: “response” (= mean response) or 
#   “se” (= standard errors and mean response). Survival: “response” (= some sort of orderable risk) 
#   or “prob” (= time dependent probabilities). Clustering: “response” (= cluster IDS) or 
#   “prob” (= fuzzy cluster membership probabilities), Multilabel: “response” (= logical matrix 
#   indicating the predicted class labels) or “prob” (= probabilities and corresponding 
#   logical matrix indicating class labels). Default is “response”.
lrn = makeLearner("classif.rpart", predict.type = "response" )

# Get a description of all possible parameter settings for a learner.
# https://rdrr.io/cran/mlr/man/getParamSet.html
# https://mlr-org.github.io/mlr-tutorial/release/html/learner/index.html
# You can also check all the tunable params. The Learner object is a list and the following 
# elements contain information regarding the hyperparameters and the type of prediction.
getParamSet(lrn)

# Nested Resampling
# https://mlr-org.github.io/mlr-tutorial/release/html/nested_resampling/index.html
makeFilterWrapper(lrn)


# Training a Learner
# Training a learner means fitting a model to a given data set. 
# In mlr this can be done by calling function train on a Learner and a suitable Task.
# Training a learner works the same way for every type of learning problem.
# Function train returns an object of class WrappedModel, which encapsulates the fitted model, 
# i.e., the output of the underlying R learning method.
# Additionally, it contains some information about the Learner, the Task, the features and 
# observations used for training, and the training time. 
# A WrappedModel can subsequently be used to make a prediction for new observations.
mod = train(lrn, rpart.task)

## Peak into mod
names(mod)
mod$learner

# Extract the fitted model ( mod$learner.model)
getLearnerModel(mod)

# Plot the fitted model
plot(getLearnerModel(mod))

pred = predict(mod, newdata = C)
head(as.data.frame(pred))

# In order to get predicted posterior probabilities we have to create a Learner 
# with the appropriate predict.type.  
lrn_prob = makeLearner("classif.rpart", predict.type = "prob")
mod_prob = train(lrn_prob, rpart.task)
pred_prob = predict(mod_prob, newdata = C)
head(as.data.frame(pred_prob))

# In addition to the probabilities, class labels are predicted by choosing the class with 
# the maximum probability and breaking ties at random.
# The predicted posterior probabilities can be accessed via the getPredictionProbabilities function.
head(getPredictionProbabilities(pred_prob))

# Confusion matrix
# https://www.rdocumentation.org/packages/mlr/versions/2.9/topics/getConfMatrix
# Calculates confusion matrix for (possibly resampled) prediction. 
# Rows indicate true classes, columns predicted classes. The marginal elements count the 
# number of classification errors for the respective row or column, i.e., the number of 
# errors when you condition on the corresponding true (rows) or predicted (columns) class. 
# The last element in the margin diagonal displays the total amount of errors. 
# Note that for resampling no further aggregation is currently performed. 
# All predictions on all test sets are joined to a vector yhat, as are all labels 
# joined to a vector y. Then yhat is simply tabulated vs y, as if both were computed 
# on a single test set. This probably mainly makes sense when cross-validation is used for resampling.
print(getConfMatrix(pred, relative = FALSE))
print(getConfMatrix(pred, relative = TRUE))

# Feature Selection
# Often, data sets include a large number of features. The technique of extracting a subset of 
# relevant features is called feature selection. Feature selection can enhance the 
# interpretability of the model, speed up the learning process and improve the learner performance. 
# There exist different approaches to identify the relevant features. mlr supports filter 
# and wrapper methods.

# Calculates feature filter values.
# https://rdrr.io/cran/mlr/man/generateFilterValuesData.html
# Calculates numerical filter values for features. For a list of features, use listFilterMethods.
# fv = generateFilterValuesData(rpart.task, method = "cforest.importance")
fv = generateFilterValuesData(rpart.task, method = "rf.importance")
print(fv)
print(fv$data)
# plotFilterValuesGGVIS(fv)
plotFilterValues(fv)
getFilteredFeatures(fv)

# Create control structures for tuning
# https://rdrr.io/cran/mlr/man/TuneControl.html
# makeTuneControlGrid - Grid search. All kinds of parameter types can be handled. 
#   You can either use their correct param type and resolution, or discretize them yourself by 
#   always using makeDiscreteParam in the par.set passed to tuneParams.
# same.resampling.instance - Should the same resampling instance be used for all evaluations to 
#   reduce variance? Default is TRUE.
# resolution - Resolution of the grid for each numeric/integer parameter in par.set. 
#   For vector parameters, it is the resolution per dimension. Either pass one resolution for all 
#   parameters, or a named vector. You can pass resolution = N if you want the algorithm to 
#   select N tune params given upper and lower bounds to a NumericParam
#   instead of a discrete one. Default is 10.
control.grid = makeTuneControlGrid(same.resampling.instance = TRUE, resolution = 10) 

# Tuning Hyperparameters
# https://mlr-org.github.io/mlr-tutorial/devel/html/tune/index.html
# Many machine learning algorithms have hyperparameters that need to be set. 
# If selected by the user they can be specified as explained on the tutorial page on Learners 
# -- simply pass them to makeLearner. Often suitable parameter values are not obvious and it is 
# preferable to tune the hyperparameters, that is automatically identify values that lead to 
# the best performance.
# We first must define a space to search when tuning our learner. 
# For example, maybe we want to tune several specific values of a hyperparameter or perhaps we 
# want to define a space from 10^{−10} to 10^{10} and let the optimization algorithm decide 
# which points to choose. In order to define a search space, we create a ParamSet object, 
# which describes the parameter space we wish to search. This is done via the function makeParamSet.
# https://www.rdocumentation.org/packages/ParamHelpers/versions/1.1-36/topics/makeParamSet?
# Multiple sets can be concatenated with c
# Create A Description Object For A Parameter, e.g.
# makeDiscreteParam(id, values, requires = NULL)
# id - Name of parameter
# values - Possible discrete values. Instead of using a vector of atomic values, you are also 
#   allowed to pass a list of quite complex R objects, which are used as discrete choices. 
# In rpart:
# minsplit - the minimum number of observations that must exist in a node in order for a split to be attempted
# cp - complexity parameter. Any split that does not decrease the overall lack of fit by a factor 
#   of cp is not attempted. For instance, with anova splitting, this means that the overall R-squared 
#   must increase by cp at each step. The main role of this parameter is to save computing time by 
#   pruning off splits that are obviously not worthwhile. Essentially,the user informs the program 
#   that any split which does not improve the fit by cp will likely be pruned off by cross-validation, 
#   and that hence the program need not pursue it.
ps = makeParamSet(
  makeDiscreteParam("cp", values = seq(0,0.2,0.1)),
  makeDiscreteParam("minsplit", values = c(30,40,50))
)
print(ps)

# Hyperparameter Tuning.
# https://www.rdocumentation.org/packages/mlr/versions/2.9/topics/tuneParams?
# Optimizes the hyperparameters of a learner. Allows for different optimization methods, 
# such as grid search, evolutionary strategies, iterated F-race, etc. 
# You can select such an algorithm (and its settings) by passing a corresponding control object. 
# tuneParams simply performs the cross-validation for every element of the cross-product and 
# selects the parameter setting with the best mean performance. 
# If no performance measure is specified, by default the error rate (mmce) is used.
# You can pass other measures and also a list of measures to tuneParams. 
# In the latter case the first measure is optimized during tuning, the others are simply evaluated. 
# If you are interested in optimizing several measures simultaneously have a look at Advanced Tuning.
# The actual tuning, with accuracy as evaluation metric
# tuneParams(learner, task, resampling, measures, par.set, control, show.info = getMlrOption("show.info"))
# learner - The learner. If you pass a string the learner will be created via makeLearner
# task - The task
# resampling - Resampling strategy to evaluate points in hyperparameter space. 
#   If you pass a description, it is instantiated once at the beginning by default, 
#   so all points are evaluated on the same training/test sets. If you want to change that behavior, 
#   look at TuneControl
# measures - Performance measures to evaluate. The first measure, aggregated by the first 
#   aggregation function is optimized, others are simply evaluated. 
#   Default is the default measure for the task, see getDefaultMeasure.
# par.set - Collection of parameters and their constraints for optimization. 
#   Dependent parameters with a requires field must use quote and not expression to define it.
# control -  Control object for search method. Also selects the optimization algorithm for tuning.
# show.info -  Print verbose output on console? Default is set via configureMlr
res = tuneParams(lrn, task = rpart.task, resampling = resamp, measures = list(acc,timetrain),
                 par.set = ps, control = control.grid, show.info = TRUE)

print(res)
print(as.data.frame(res$opt.path))

# https://mlr-org.github.io/mlr-tutorial/devel/html/hyperpar_tuning_effects/index.html
names(res)
print(c(res$x$cp, res$x$minsplit))
print(res$y)

# Investigating hyperparameter tuning effects
# https://www.rdocumentation.org/packages/mlr/versions/2.9/topics/generateHyperParsEffectData?
generateHyperParsEffectData(res, include.diagnostics = TRUE)
```



<!-- regular html comment 


--> 



