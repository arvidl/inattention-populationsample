---
title: "Prediction of academic achievement in adolescents from teacher reports of inattention in childhood - a methodological pattern classification study"
output: html_notebook
---
```{r, echo=TRUE, eval=FALSE}
~GitHub/inattention-population/code/inattention-populationsample-mlr.Rmd
```
<small>
This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 
Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 
Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.
When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file).
</small>


## Abstract

### Background
Inattentive behavior is associated with academic problems. The present study investigates primary school teacher reports on nine items reflecting different aspects of inattention, with an aim to reveal patterns of behavior predicting high-school academic achievement. To that end, we used different types of pattern analysis and machine learning methods. 

### Methods
Inattention in a sample 2397 individuals were rated by their primary school teachers when they participated in the first wave of the Bergen Child Study (BCS) (7 - 9 years old), and their academic achievements were available from an official school register when attending high-school (16 - 19 years old). Inattention was assessed by the nine items rated at a categorical leve, and the academic achievement scores were divided into three parts including a similar number of participants. 

### Results 
Boys obtained higher inattention scores and lower academic scores than girls. Inattention problems related to sustained attention and distractibility turned out to have the highest predictive value of academic achievement level across all selected statistical analyses, and the full model showed that inattention explained about 10\% of the variance in high school scores about 10 years later. A high odds-ration of being allocated to the lowest academic achievement category was shown by a multinominal regression analysis, while a pattern of problems related to sustained attention and distractibility was revealed by generating classification trees. By including recursive learning algorithms, the most successful classification was found between these inattention items and the highest level of achievement scores. 

### Summary 
The present study showed the importance of a pattern of early problems related to sustained attention and distractibility in predicting future academic results. By including different statistical classification models we showed that this pattern was fairly consistent. Furthermore, calculation of classification errors gave information about the uncertainty when predicting the outcome for individual children. Further studies should include a wider range of variables. 




<small>Organization of the data and the analysis:</small>

Libraries being used:

* memisc - spss.system.file()
* psych  - headTail(), describe()
* Hmisc - describe()
* pander - pander(), panderOptions()

<img src="../images/Data_to_classes_20160205_pptx.jpg" width="500px" height="500px" />

### Data preparation

Input file:

 * inattention_Arvid_new.sav (from Astri, on ~/Dropbox/Arvid_inatteion/data2)
 * inattention_nomiss_2397x12.csv
 
Output files (data):

 * inattention_nomiss_2397x12_snap_is_0_1_2.csv
 * inattention_nomiss_2397x12_snap_is_0_1.csv
 * inattention_nomiss_2397x12_snap_is_0_1_2_outcome_is_L_M_H.csv (Low, Medium, High academic score)
 * inattention_nomiss_2397x12_snap_is_0_1_2_outcome_is_0_1_2.csv (all numerical)
 * inattention_nomiss_2397x12_snap_is_N_S_C_outcome_is_L_M_H.csv (Not, Somewhat, Certainly true)
 

```{r, echo=TRUE, eval=TRUE} 
fn <- "../data2/inattention_Arvid_new.sav"
```

```{r, echo=TRUE, eval=FALSE}
# The original SPSS file as provided to AJL is
# 'inattention_Astri_94_96_new_grades_updated.sav'
# and being edited and reduced by AJL to 'inattention_Arvid_new.sav'
# Import data stored in the SPSS format
library(memisc)
fn <- "../data2/inattention_Arvid_new.sav"
data <- as.data.set(spss.system.file(fn))

# Make new data frame from the sample with the variables 
# gender, grade, SNAP1, ..., SNAP9 (vars #1-11) and
# academic_achievement (var #52) 
names(data)
d <- data[, c(1:11, 52)]
dim(d)
names(d)
str(d)
summary(d)
```

```{r, echo=TRUE, eval=TRUE}
D3 <- read.csv(file = "../data/inattention_nomiss_2397x12_snap_is_0_1_2.csv") 
C <- read.csv(file = "../data/inattention_nomiss_2397x12_snap_is_0_1_2_outcome_is_L_M_H.csv")
D <- read.csv(file = "../data/inattention_nomiss_2397x12_snap_is_N_S_C_outcome_is_L_M_H.csv")
E <- read.csv(file = "../data/inattention_nomiss_2397x12_snap_is_0_1_2_outcome_is_0_1_2.csv")
str(D3)
head(D3)
str(C)
head(C)
str(D)
head(D)
str(E)
head(E)
```



```{r, echo=TRUE, eval=TRUE}
E$averBinnedF = as.factor(E$averBinned)  # To perform clasification and not regression
E <- subset(E, select = -c(averBinned))
head(E)
```


### More on the SNAP data

Use Likert scale with three levels (not collapsing to two) 

APROPOS:
- should actually be five (https://en.wikipedia.org/wiki/Likert_scale)
When responding to a Likert questionnaire item, respondents specify their level of agreement 
or disagreement on a symmetric agree-disagree scale for a series of statements. 
Thus, the range captures the intensity of their feelings for a given item.
The format of a typical five-level Likert item, for example, could be:

 * 1 Strongly disagree
 * 2 Disagree
 * 3 Neither agree nor disagree
 * 4 Agree
 * 5 Strongly agree

Likert scaling is a bipolar scaling method, measuring either positive or negative response to a statement.

```{r, echo=TRUE, eval=TRUE}
data <- as.data.frame(E)
# for(i in 1:9){
#  cmd = sprintf("data$snap%d <- EE$snap%d", i, i)
#  # print(cmd)
#  eval(parse(text=cmd))
#}
```

```{r, echo=TRUE, eval=TRUE}
# Check that no datapoint is missing
apply(data,2,function(x) sum(is.na(x)))
```

```{r, echo=TRUE, eval=TRUE}
# SIDESTEP: Randomly splitting the data into a train and a test set
index <- sample(1:nrow(data),round(0.75*nrow(data)))
train <- data[index,]
test <- data[-index,]

# Fit a linear regression model and test it on the test set. 
# lm.fit <- glm(averBinned ~ ., data=train)
# summary(lm.fit)
```


#### Using *multinom* in the NNET package

```{r, echo=TRUE, eval=TRUE}
# Before running our model. We then choose the level of our outcome that we wish 
# to use as our baseline and specify this in the relevel function. 
# Then, we run our model using multinom.

library(nnet)
data$AverageMarksLevel3 <- relevel(factor(data$averBinnedF), ref = "2")   # ref = "high"
multinom.E.1 <- multinom(AverageMarksLevel3 ~ 
                   gender+grade +
                   snap1+snap2+snap3+snap4+snap5+snap6+snap7+snap8+snap9,
                   data = data)
multinom.E.1
summary(multinom.E.1)
z <- summary(multinom.E.1)$coefficients/summary(multinom.E.1)$standard.errors
# 2-tailed z test
p <- (1 - pnorm(abs(z), 0, 1)) * 2
p

## extract the coefficients from the model and exponentiate
c <- exp(coef(multinom.E.1))


# You can also use predicted probabilities to help you understand the model. 
# Calculate predicted probabilities for each of the outcome levels using 
# the fitted function. We can start by generating the predicted probabilities 
# for the observations in our dataset and viewing the first few rows
head(pp <- fitted(multinom.E.1))
```

```{r, echo=TRUE, eval=TRUE}
# Fore easy input to LaTeX:
library(stargazer)
stargazer(multinom.E.1)
stargazer(p)
stargazer(c)
```




### Using the CARET / NNET package

The caret package (short for classification and regression training) contains functions to
streamline the model training process for complex regression and classification problems.
The package utilizes a number of R packages but tries not to load them all at package
start-up. The package “suggests” field includes 27 packages. caret loads packages as
needed and assumes that they are installed. See http://topepo.github.io/caret/Logistic_Regression.html

Penalized Multinomial Regression - method = 'multinom'

See e.g. http://www.ats.ucla.edu/stat/r/dae/mlogit.htm
and ml <- read.dta("http://www.ats.ucla.edu/stat/data/hsbdemo.dta")
Multinomial logistic regression is used to model nominal outcome variables,
in which the log odds of the outcomes are modeled as a linear combination 
of the predictor variables.
See also: Agresti_Foundations_of_Linear_and_Generalized_Linear_Models_Wiley_2015.pdf (Chap.6)
To report statistical results using R, e.g. MLR see
Andy Field et al.  Discovering Statistics Using R. Sage Publishing 2012.
http://studysites.uk.sagepub.com/dsur/main.htm and R code in
http://studysites.uk.sagepub.com/dsur/study/scriptfi.htm




```{r, echo=TRUE, eval=TRUE}
library(caret)
# Feature density plots
library(AppliedPredictiveModeling)
transparentTheme(trans = 0.4)

# plt <- featurePlot(x = E[, 1:11],
featurePlot(x = E[, 1:11],
            y = E$averBinnedF,
            plot = "density",
            scales = list(x = list(relation="free"),
                          y = list(relation="free")),
            pch = "|",
            adjust = 1.5,
            layout = c(4,3),
            ## Add a key at the top
            auto.key = list(columns = 2))
# plot(plt)
```

```{r, echo=TRUE, eval=TRUE}
# Functions and Datasets from J. Fox and S. Weisberg, An R Companion to Applied Regression,
# Second Edition, Sage, 2011.
library(car)

# Keep 75 (70) % of the observations for training the classifier, 
# and 25 (30) % of the sample for performance evaluation (e.g. confusion matrix)
set.seed(998)
trainIndex <- createDataPartition(E$averBinnedF, p=.75, list=F)
# trainIndex <- createDataPartition(data$class, p=.70, list=F)
E.train <- E[trainIndex, ]
E.test <- E[-trainIndex, ]

# Check the random sample used for training
head(trainIndex)
```

#### Using an alternative implementation of MLR - mlogit

```{r, echo=TRUE, eval=TRUE}
library(mlogit)

# We need to modify the data so that the multinomial logistic regression
# function can process it. To do this, we need to expand the outcome variable
# (y) much like we would for dummy coding a categorical variable for
# inclusion in standard multiple regression.
head(E)
E2 <- mlogit.data(E, varying=NULL, choice="averBinnedF", shape="wide")
head(E2)

# Now we can proceed with the multinomial logistic regression analysis using
# the ‘mlogit’ function and the ubiquitous ‘summary’ function of the results.
# Note that the reference category is specified as “high” (2, where the levels are 0,1,2).
mlogit.E.1 <- mlogit(averBinnedF ~ 1 | gender+grade +
                    snap1+snap2+snap3+snap4+snap5+snap6+snap7+snap8+snap9,
                    data = E2,
                    reflevel="2")    # reflevel = "high")
mlogit.E.1
summary(mlogit.E.1)

b <- exp(coef(mlogit.E.1))
b
summary(b)

# https://www.r-bloggers.com/how-to-get-the-frequency-table-of-a-categorical-variable-as-a-data-frame-in-r/
ci <- confint(mlogit.E.1, level=0.95)
print(ci)
```

#### How to plot ROC curves in multiclass classification?

http://stats.stackexchange.com/questions/2151/how-to-plot-roc-curves-in-multiclass-classification
install.packages("pROC")
I recently found this [pROC](http://web.expasy.org/pROC) package in R which plots a multiclass ROC using the technique specified by Hand and Till (2001). You can use the multiclass.roc function.


### Example from UCLA Statistical Consulting Group
#### R Data Analysis Examples: Multinomial Logistic Regression

[mlogit](http://www.ats.ucla.edu/stat/r/dae/mlogit.htm)

Multinomial logistic regression is used to model nominal outcome variables, in which the log odds of the outcomes are modeled as a linear combination of the predictor variables.

This page uses the following packages. Make sure that you can load them before trying to run the examples on this page. If you do not have a package installed, run: install.packages("packagename"), or if you see the version is out of date, run: update.packages().

```{r, echo=TRUE, eval=TRUE}
library(foreign)
library(nnet)
library(ggplot2)
library(reshape2)
```
*Please note:* The purpose of this page is to show how to use various data analysis commands. It does not cover all aspects of the research process which researchers are expected to do. In particular, it does not cover data cleaning and checking, verification of assumptions, model diagnostics or potential follow-up analyses.

#### Examples of multinomial logistic regression

*Example 1.* People's occupational choices might be influenced by their parents' occupations and their own education level. We can study the relationship of one's occupation choice with education level and father's occupation. The occupational choices will be the outcome variable which consists of categories of occupations.

*Example 2*. A biologist may be interested in food choices that alligators make. Adult alligators might have different preferences from young ones. The outcome variable here will be the types of food, and the predictor variables might be size of the alligators and other environmental variables.

*Example 3.* Entering high school students make program choices among general program, vocational program and academic program. Their choice might be modeled using their writing score and their social economic status.

#### Description of the data

```{r, echo=TRUE, eval=TRUE}
ml <- read.dta("http://www.ats.ucla.edu/stat/data/hsbdemo.dta")
```
The data set contains variables on 200 students. The outcome variable is prog, program type. The predictor variables are social economic status, ses, a three-level categorical variable and writing score, write, a continuous variable. Let's start with getting some descriptive statistics of the variables of interest.

```{r, echo=TRUE, eval=TRUE}
with(ml, table(ses, prog))
```


```{r, echo=TRUE, eval=TRUE}
with(ml, do.call(rbind, tapply(write, prog,
  function(x) c(M = mean(x), SD = sd(x)))))
```

#### Analysis methods you might consider

 * Multinomial logistic regression, the focus of this page.

 * Multinomial probit regression, similar to multinomial logistic regression with independent normal error terms.

 * Multiple-group discriminant function analysis. A multivariate method for multinomial outcome variables

 * Multiple logistic regression analyses, one for each pair of outcomes: One problem with this approach is that each analysis is potentially run on a different sample. The other problem is that without constraining the logistic models, we can end up with the probability of choosing all possible outcome categories greater than 1.

 * Collapsing number of categories to two and then doing a logistic regression: This approach suffers from loss of information and changes the original research questions to very different ones.

 * Ordinal logistic regression: If the outcome variable is truly ordered and if it also satisfies the assumption of proportional odds, then switching to ordinal logistic regression will make the model more parsimonious.

 * Alternative-specific multinomial probit regression, which allows different error structures therefore allows to relax the IIA assumption. This requires that the data structure be choice-specific.

 * Nested logit model, another way to relax the IIA assumption, also requires the data structure be choice-specific.


#### Multinomial logistic regression

Below we use the multinom function from the nnet package to estimate a multinomial logistic regression model. There are other functions in other R packages capable of multinomial regression. We chose the multinom function because it does not require the data to be reshaped (as the mlogit package does) and to mirror the example code found in Hilbe's Logistic Regression Models.

Before running our model it is important to choose a refernce group for our outcome. We can choose the level of our outcome that we wish to use as our baseline and specify this in the relevel function. Then, we run our model using multinom. The multinom package does not include p-value calculation for the regression coefficients, so we calculate p-values using Wald tests (here z-tests).

```{r, echo=TRUE, eval=TRUE}
ml$prog2 <- relevel(ml$prog, ref = "academic")
test <- multinom(prog2 ~ ses + write, data = ml)
```


```{r, echo=TRUE, eval=TRUE}
summary(test)
```


```{r, echo=TRUE, eval=TRUE}
z <- summary(test)$coefficients/summary(test)$standard.errors
print(z)
```


```{r, echo=TRUE, eval=TRUE}
#2-tailed z test
p <- (1 - pnorm(abs(z), 0, 1))*2
print(p)
```

The ratio of the probability of choosing one outcome category over the probability of choosing the baseline category is often referred as relative risk (and it is also sometimes referred as odds as we have just used to described the regression parameters above). The relative risk is the right-hand side linear equation exponentiated, leading to the fact that the exponentiated regression coefficients are relative risk ratios for a unit change in the predictor variable. We can exponentiate the coefficients from our model to see these risk ratios.

```{r, echo=TRUE, eval=TRUE}
## extract the coefficients from the model and exponentiate
exp(coef(test))
```

You can also use predicted probabilities to help you understand the model. You can calculate predicted probabilities for each of our outcome levels using the fitted function. We can start by generating the predicted probabilities for the observations in our dataset and viewing the first few rows
```{r, echo=TRUE, eval=TRUE}
head(pp <- fitted(test))
```

Next, if we want to examine the changes in predicted probability associated with one of our two variables, we can create small datasets varying one variable while holding the other constant. We will first do this holding write at its mean and examining the predicted probabilities for each level of ses.
```{r, echo=TRUE, eval=TRUE}
dses <- data.frame(ses = c("low", "middle", "high"), write = mean(ml$write))
predict(test, newdata = dses, "probs")
```

Another way to understand the model using the predicted probabilities is to look at the averaged predicted probabilities for different values of the continuous predictor variable write within each level of *ses*.
```{r, echo=TRUE, eval=FALSE}
#dwrite <- data.frame(ses = rep(c("low", "middle", "high"), each = 41), write = rep(c(30:70), 3)
# store the predicted probabilities for each value of ses and write
pp.write <- cbind(dwrite, predict(test, newdata = dwrite, type = "probs", se = TRUE))

## calculate the mean probabilities within each level of ses
by(pp.write[, 3:5], pp.write$ses, colMeans)
```

Sometimes, a couple of plots can convey a good deal amount of information. Using the predictions we generated for the pp.write object above, we can plot the predicted probabilities against the writing score by the level of ses for different levels of the outcome variable.
```{r, echo=TRUE, eval=FALSE}
## melt data set to long for ggplot2
lpp <- melt(pp.write, id.vars = c("ses", "write"), value.name = "probability")
head(lpp) # view first few rows
```

```{r, echo=TRUE, eval=FALSE}
## plot predicted probabilities across write values for
## each level of ses facetted by program type
ggplot(lpp, aes(x = write, y = probability, colour = ses)) +
  geom_line() +
  facet_grid(variable ~ ., scales="free")
``` 


#### Things to consider

 * The Independence of Irrelevant Alternatives (IIA) assumption: Roughly, the IIA assumption means that adding or deleting alternative outcome categories does not affect the odds among the remaining outcomes. There are alternative modeling methods, such as alternative-specific multinomial probit model, or nested logit model to relax the IIA assumption.

 * Diagnostics and model fit: Unlike logistic regression where there are many statistics for performing model diagnostics, it is not as straightforward to do diagnostics with multinomial logistic regression models. For the purpose of detecting outliers or influential data points, one can run separate logit models and use the diagnostics tools on each model.

 * Sample size: Multinomial regression uses a maximum likelihood estimation method, it requires a large sample size. It also uses multiple equations. This implies that it requires an even larger sample size than ordinal or binary logistic regression.

 * Complete or quasi-complete separation: Complete separation means that the outcome variable separate a predictor variable completely, leading perfect prediction by the predictor variable.

 * Perfect prediction means that only one value of a predictor variable is associated with only one value of the response variable. But you can tell from the output of the regression coefficients that something is wrong. You can then do a two-way tabulation of the outcome variable with the problematic variable to confirm this and then rerun the model without the problematic variable.

 * Empty cells or small cells: You should check for empty or small cells by doing a cross-tabulation between categorical predictors and the outcome variable. If a cell has very few cases (a small cell), the model may become unstable or it might not even run at all.



  
<!-- regular html comment

Train the MLR or RF or ANN model on the training set (caret package)
check: to http://cran.r-project.org/web/packages/nnet/nnet.pdf, and
http://topepo.github.io/caret/training.html
The function train() sets up a grid of tuning parameters for a number of
classification and regression routines, fits each model and calculates
a resampling based performance measure.
method - a string specifying which classification or regression model to use
see http://topepo.github.io/caret/bytag.html
and https://www.byclb.com/TR/Tutorials/neural_networks/ch10_1.htm !!

```{r, echo=TRUE, eval=TRUE}
# The function trainControl can be used to specifiy the type of resampling in the training set
fitControl <- trainControl(   ## 10-fold CV [cross validation]
  method = "repeatedcv",
  number = 10,
  ## repeated ten times
  repeats = 10)

# Preprocessing (normalization) of training data
# Check: https://github.com/topepo/caret/issues/160
pp.E.train <- preProcess(E.train, method = c("center", "scale"))
pp.E.train <- predict.preProcess(pp.E.train, E.train)
# Preprocessing (normalization) of test data
pp.E.test <- preProcess(E.test, method = c("center", "scale"))
pp.E.test <- predict.preProcess(pp.E.test, E.test)
head(pp.E.train)
head(E.train)
head(pp.E.test)
head(E.test)
```


```{r, echo=TRUE, eval=TRUE}
# Neural Network classifier  cf. help(nnet)
# method = 'nnet'
# Type: Classification, Regression
# Tuning Parameters: size (#Hidden Units), decay (Weight Decay)

# For these caret models, train can automatically create a grid of tuning parameters.
# By default, if p is the number of tuning parameters, the grid size is 3^p.
# For nnet we explore
my.nnet.grid <- expand.grid(.decay = c(0.05, 0.10, 0.15, 0.20, 0.25), .size = c(3, 5, 7, 9))

E.nnet.fit <- train(averBinnedF ~. , data = E.train,
                       method = "nnet",
                       maxit = 1000,
                       trControl = fitControl,
                       tuneGrid = my.nnet.grid,
                       trace = FALSE)

print(E.nnet.fit$finalModel$problemType)
print(E.nnet.fit$finalModel$n)
print(E.nnet.fit$finalModel$call)
print(E.nnet.fit$finalModel)
print(E.nnet.fit)

trellis.par.set(caretTheme())
plot(E.nnet.fit)
```


```{r, echo=TRUE, eval=TRUE}
# Call predict on the fitted object using the test data set
E.test.nnet.predict <- predict(E.nnet.fit, newdata = E.test, na.action = na.omit, verbose = TRUE)
print(summary(as.data.frame(E.test.nnet.predict)))
print(summary(as.data.frame(E.test$averBinnedF)))

# Assess confusion matrix
cm.nnet.fit.E.test <- confusionMatrix(E.test.nnet.predict, E.test$averBinnedF)
print(cm.nnet.fit.E.test)
```



<!-- regular html comment

###  Explore the performance of another (Support Vector Machines with Radial Basis Function Kernel) classifier - svmRadial

```{r, echo=TRUE, eval=TRUE}
library(plyr)
#mapvalues(E$averBinnedF, from = c("0", "1", "2"), to = c("L", "M", "H"))
#revalue(E$averBinnedF, c("0"="L", "1"="M", "2"="H"))
levels(E$averBinnedF)[levels(E$averBinnedF)=="0"] <- "L"
levels(E$averBinnedF)[levels(E$averBinnedF)=="1"] <- "M"
levels(E$averBinnedF)[levels(E$averBinnedF)=="2"] <- "H"

set.seed(998)
trainIndex <- createDataPartition(E$averBinnedF, p=.75, list=F)
# trainIndex <- createDataPartition(data$class, p=.70, list=F)
E.train <- E[trainIndex, ]
E.test <- E[-trainIndex, ]
```

```{r, echo=TRUE, eval=TRUE}
fitControl <- trainControl(   ## 10-fold CV [cross validation]
  method = "repeatedcv",
  number = 10,
  ## repeated ten times
  repeats = 10,
  classProbs = TRUE)
```

```{r, echo=TRUE, eval=TRUE}
svmFit <- train(averBinnedF ~ . , data = E.train,
                 method = "svmRadial",
                 trControl = fitControl,
                 tuneLength = 8,
                 metric = "ROC")
svmFit
```

```{r, echo=TRUE, eval=TRUE}
# Call predict on the fitted object using the test data set
E.test.svm.predict <- predict(svmFit, newdata = E.test, na.action = na.omit, verbose = TRUE)
print(summary(as.data.frame(E.test.svm.predict)))
print(summary(as.data.frame(E.test$averBinnedF)))

# Assess confusion matrix
cm.svm.fit.E.test <- confusionMatrix(E.test.svm.predict, E.test$averBinnedF)
print(cm.svm.fit.E.test)
```

--> 



