---
title: "Prediction of academic achievement in adolescents from teacher reports of inattention in childhood - a methodological pattern classification study"
output: html_notebook
---
```{r, echo=TRUE, eval=FALSE}
~GitHub/inattention-population/code/inattention-populationsample-mlr.Rmd
```
<small>
This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 
Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 
Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.
When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file).
</small>


## Abstract

### Background
Inattentive behavior is associated with academic problems. The present study investigates primary school teacher reports on nine items reflecting different aspects of inattention, with an aim to reveal patterns of behavior predicting high-school academic achievement. To that end, we used different types of pattern analysis and machine learning methods. 

### Methods
Inattention in a sample 2397 individuals were rated by their primary school teachers when they participated in the first wave of the Bergen Child Study (BCS) (7 - 9 years old), and their academic achievements were available from an official school register when attending high-school (16 - 19 years old). Inattention was assessed by the nine items rated at a categorical leve, and the academic achievement scores were divided into three parts including a similar number of participants. 

### Results 
Boys obtained higher inattention scores and lower academic scores than girls. Inattention problems related to sustained attention and distractibility turned out to have the highest predictive value of academic achievement level across all selected statistical analyses, and the full model showed that inattention explained about 10\% of the variance in high school scores about 10 years later. A high odds-ration of being allocated to the lowest academic achievement category was shown by a multinominal regression analysis, while a pattern of problems related to sustained attention and distractibility was revealed by generating classification trees. By including recursive learning algorithms, the most successful classification was found between these inattention items and the highest level of achievement scores. 

### Summary 
The present study showed the importance of a pattern of early problems related to sustained attention and distractibility in predicting future academic results. By including different statistical classification models we showed that this pattern was fairly consistent. Furthermore, calculation of classification errors gave information about the uncertainty when predicting the outcome for individual children. Further studies should include a wider range of variables. 




<small>Organization of the data and the analysis:</small>

Libraries being used:

* memisc - spss.system.file()
* psych  - headTail(), describe()
* Hmisc - describe()
* pander - pander(), panderOptions()

<img src="../images/Data_to_classes_20160205_pptx.jpg" width="500px" height="500px" />

### Data preparation

Input file:

 * inattention_Arvid_new.sav (from Astri, on ~/Dropbox/Arvid_inatteion/data2)
 * inattention_nomiss_2397x12.csv
 
Output files (data):

 * inattention_nomiss_2397x12_snap_is_0_1_2.csv
 * inattention_nomiss_2397x12_snap_is_0_1.csv
 * inattention_nomiss_2397x12_snap_is_0_1_2_outcome_is_L_M_H.csv (Low, Medium, High academic score)
 * inattention_nomiss_2397x12_snap_is_0_1_2_outcome_is_0_1_2.csv (all numerical)
 * inattention_nomiss_2397x12_snap_is_N_S_C_outcome_is_L_M_H.csv (Not, Somewhat, Certainly true)
 

```{r, echo=TRUE, eval=TRUE} 
fn <- "../data2/inattention_Arvid_new.sav"
```

```{r, echo=TRUE, eval=FALSE}
# The original SPSS file as provided to AJL is
# 'inattention_Astri_94_96_new_grades_updated.sav'
# and being edited and reduced by AJL to 'inattention_Arvid_new.sav'
# Import data stored in the SPSS format
library(memisc)
fn <- "../data2/inattention_Arvid_new.sav"
data <- as.data.set(spss.system.file(fn))

# Make new data frame from the sample with the variables 
# gender, grade, SNAP1, ..., SNAP9 (vars #1-11) and
# academic_achievement (var #52) 
names(data)
d <- data[, c(1:11, 52)]
dim(d)
names(d)
str(d)
summary(d)
```

```{r, echo=TRUE, eval=TRUE}
D3 <- read.csv(file = "../data/inattention_nomiss_2397x12_snap_is_0_1_2.csv") 
C <- read.csv(file = "../data/inattention_nomiss_2397x12_snap_is_0_1_2_outcome_is_L_M_H.csv")
D <- read.csv(file = "../data/inattention_nomiss_2397x12_snap_is_N_S_C_outcome_is_L_M_H.csv")
E <- read.csv(file = "../data/inattention_nomiss_2397x12_snap_is_0_1_2_outcome_is_0_1_2.csv")
str(D3)
head(D3)
str(C)
head(C)
str(D)
head(D)
str(E)
head(E)
```



```{r, echo=TRUE, eval=TRUE}
E$averBinnedF = as.factor(E$averBinned)  # To perform clasification and not regression
E <- subset(E, select = -c(averBinned))
head(E)
```


### More on the SNAP data

Use Likert scale with three levels (not collapsing to two) 

APROPOS:
- should actually be five (https://en.wikipedia.org/wiki/Likert_scale)
When responding to a Likert questionnaire item, respondents specify their level of agreement 
or disagreement on a symmetric agree-disagree scale for a series of statements. 
Thus, the range captures the intensity of their feelings for a given item.
The format of a typical five-level Likert item, for example, could be:

 * 1 Strongly disagree
 * 2 Disagree
 * 3 Neither agree nor disagree
 * 4 Agree
 * 5 Strongly agree

Likert scaling is a bipolar scaling method, measuring either positive or negative response to a statement.

```{r, echo=TRUE, eval=TRUE}
data <- as.data.frame(E)
# for(i in 1:9){
#  cmd = sprintf("data$snap%d <- EE$snap%d", i, i)
#  # print(cmd)
#  eval(parse(text=cmd))
#}
```

```{r, echo=TRUE, eval=TRUE}
# Check that no datapoint is missing
apply(data,2,function(x) sum(is.na(x)))
```

```{r, echo=TRUE, eval=TRUE}
# SIDESTEP: Randomly splitting the data into a train and a test set
index <- sample(1:nrow(data),round(0.75*nrow(data)))
train <- data[index,]
test <- data[-index,]

# Fit a linear regression model and test it on the test set. 
# lm.fit <- glm(averBinned ~ ., data=train)
# summary(lm.fit)
```


#### Using *multinom* in the NNET package

```{r, echo=TRUE, eval=TRUE}
# Before running our model. We then choose the level of our outcome that we wish 
# to use as our baseline and specify this in the relevel function. 
# Then, we run our model using multinom.

library(nnet)
data$AverageMarksLevel3 <- relevel(factor(data$averBinnedF), ref = "2")   # ref = "high"
multinom.E.1 <- multinom(AverageMarksLevel3 ~ 
                   gender+grade +
                   snap1+snap2+snap3+snap4+snap5+snap6+snap7+snap8+snap9,
                   data = data)
multinom.E.1
summary(multinom.E.1)
z <- summary(multinom.E.1)$coefficients/summary(multinom.E.1)$standard.errors
# 2-tailed z test
p <- (1 - pnorm(abs(z), 0, 1)) * 2
p

## extract the coefficients from the model and exponentiate
c <- exp(coef(multinom.E.1))


# You can also use predicted probabilities to help you understand the model. 
# Calculate predicted probabilities for each of the outcome levels using 
# the fitted function. We can start by generating the predicted probabilities 
# for the observations in our dataset and viewing the first few rows
head(pp <- fitted(multinom.E.1))
```

```{r, echo=TRUE, eval=TRUE}
# Fore easy input to LaTeX:
library(stargazer)
stargazer(multinom.E.1)
stargazer(p)
stargazer(c)
```




### Using the CARET / NNET package

The caret package (short for classification and regression training) contains functions to
streamline the model training process for complex regression and classification problems.
The package utilizes a number of R packages but tries not to load them all at package
start-up. The package “suggests” field includes 27 packages. caret loads packages as
needed and assumes that they are installed. See http://topepo.github.io/caret/Logistic_Regression.html

Penalized Multinomial Regression - method = 'multinom'

See e.g. http://www.ats.ucla.edu/stat/r/dae/mlogit.htm
and ml <- read.dta("http://www.ats.ucla.edu/stat/data/hsbdemo.dta")
Multinomial logistic regression is used to model nominal outcome variables,
in which the log odds of the outcomes are modeled as a linear combination 
of the predictor variables.
See also: Agresti_Foundations_of_Linear_and_Generalized_Linear_Models_Wiley_2015.pdf (Chap.6)
To report statistical results using R, e.g. MLR see
Andy Field et al.  Discovering Statistics Using R. Sage Publishing 2012.
http://studysites.uk.sagepub.com/dsur/main.htm and R code in
http://studysites.uk.sagepub.com/dsur/study/scriptfi.htm




```{r, echo=TRUE, eval=TRUE}
library(caret)
# Feature density plots
library(AppliedPredictiveModeling)
transparentTheme(trans = 0.4)

# plt <- featurePlot(x = E[, 1:11],
featurePlot(x = E[, 1:11],
            y = E$averBinnedF,
            plot = "density",
            scales = list(x = list(relation="free"),
                          y = list(relation="free")),
            pch = "|",
            adjust = 1.5,
            layout = c(4,3),
            ## Add a key at the top
            auto.key = list(columns = 2))
# plot(plt)
```

```{r, echo=TRUE, eval=TRUE}
# Functions and Datasets from J. Fox and S. Weisberg, An R Companion to Applied Regression,
# Second Edition, Sage, 2011.
library(car)

# Keep 75 (70) % of the observations for training the classifier, 
# and 25 (30) % of the sample for performance evaluation (e.g. confusion matrix)
set.seed(998)
trainIndex <- createDataPartition(E$averBinnedF, p=.75, list=F)
# trainIndex <- createDataPartition(data$class, p=.70, list=F)
E.train <- E[trainIndex, ]
E.test <- E[-trainIndex, ]

# Check the random sample used for training
head(trainIndex)
```

#### Using an alternative implementation of MLR - mlogit

```{r, echo=TRUE, eval=TRUE}
library(mlogit)

# We need to modify the data so that the multinomial logistic regression
# function can process it. To do this, we need to expand the outcome variable
# (y) much like we would for dummy coding a categorical variable for
# inclusion in standard multiple regression.
head(E)
E2 <- mlogit.data(E, varying=NULL, choice="averBinnedF", shape="wide")
head(E2)

# Now we can proceed with the multinomial logistic regression analysis using
# the ‘mlogit’ function and the ubiquitous ‘summary’ function of the results.
# Note that the reference category is specified as “high” (2, where the levels are 0,1,2).
mlogit.E.1 <- mlogit(averBinnedF ~ 1 | gender+grade +
                    snap1+snap2+snap3+snap4+snap5+snap6+snap7+snap8+snap9,
                    data = E2,
                    reflevel="2")    # reflevel = "high")
mlogit.E.1
summary(mlogit.E.1)

b <- exp(coef(mlogit.E.1))
b
summary(b)

```


<!-- regular html comment

Train the MLR or RF or ANN model on the training set (caret package)
check: to http://cran.r-project.org/web/packages/nnet/nnet.pdf, and
http://topepo.github.io/caret/training.html
The function train() sets up a grid of tuning parameters for a number of
classification and regression routines, fits each model and calculates
a resampling based performance measure.
method - a string specifying which classification or regression model to use
see http://topepo.github.io/caret/bytag.html
and https://www.byclb.com/TR/Tutorials/neural_networks/ch10_1.htm !!

```{r, echo=TRUE, eval=TRUE}
# The function trainControl can be used to specifiy the type of resampling in the training set
fitControl <- trainControl(   ## 10-fold CV [cross validation]
  method = "repeatedcv",
  number = 10,
  ## repeated ten times
  repeats = 10)

# Preprocessing (normalization) of training data
# Check: https://github.com/topepo/caret/issues/160
pp.E.train <- preProcess(E.train, method = c("center", "scale"))
pp.E.train <- predict.preProcess(pp.E.train, E.train)
# Preprocessing (normalization) of test data
pp.E.test <- preProcess(E.test, method = c("center", "scale"))
pp.E.test <- predict.preProcess(pp.E.test, E.test)
head(pp.E.train)
head(E.train)
head(pp.E.test)
head(E.test)
```


```{r, echo=TRUE, eval=TRUE}
# Neural Network classifier  cf. help(nnet)
# method = 'nnet'
# Type: Classification, Regression
# Tuning Parameters: size (#Hidden Units), decay (Weight Decay)

# For these caret models, train can automatically create a grid of tuning parameters.
# By default, if p is the number of tuning parameters, the grid size is 3^p.
# For nnet we explore
my.nnet.grid <- expand.grid(.decay = c(0.05, 0.10, 0.15, 0.20, 0.25), .size = c(3, 5, 7, 9))

E.nnet.fit <- train(averBinnedF ~. , data = E.train,
                       method = "nnet",
                       maxit = 1000,
                       trControl = fitControl,
                       tuneGrid = my.nnet.grid,
                       trace = FALSE)

print(E.nnet.fit$finalModel$problemType)
print(E.nnet.fit$finalModel$n)
print(E.nnet.fit$finalModel$call)
print(E.nnet.fit$finalModel)
print(E.nnet.fit)

trellis.par.set(caretTheme())
plot(E.nnet.fit)
```


```{r, echo=TRUE, eval=TRUE}
# Call predict on the fitted object using the test data set
E.test.nnet.predict <- predict(E.nnet.fit, newdata = E.test, na.action = na.omit, verbose = TRUE)
print(summary(as.data.frame(E.test.nnet.predict)))
print(summary(as.data.frame(E.test$averBinnedF)))

# Assess confusion matrix
cm.nnet.fit.E.test <- confusionMatrix(E.test.nnet.predict, E.test$averBinnedF)
print(cm.nnet.fit.E.test)
```



<!-- regular html comment

###  Explore the performance of another (Support Vector Machines with Radial Basis Function Kernel) classifier - svmRadial

```{r, echo=TRUE, eval=TRUE}
library(plyr)
#mapvalues(E$averBinnedF, from = c("0", "1", "2"), to = c("L", "M", "H"))
#revalue(E$averBinnedF, c("0"="L", "1"="M", "2"="H"))
levels(E$averBinnedF)[levels(E$averBinnedF)=="0"] <- "L"
levels(E$averBinnedF)[levels(E$averBinnedF)=="1"] <- "M"
levels(E$averBinnedF)[levels(E$averBinnedF)=="2"] <- "H"

set.seed(998)
trainIndex <- createDataPartition(E$averBinnedF, p=.75, list=F)
# trainIndex <- createDataPartition(data$class, p=.70, list=F)
E.train <- E[trainIndex, ]
E.test <- E[-trainIndex, ]
```

```{r, echo=TRUE, eval=TRUE}
fitControl <- trainControl(   ## 10-fold CV [cross validation]
  method = "repeatedcv",
  number = 10,
  ## repeated ten times
  repeats = 10,
  classProbs = TRUE)
```

```{r, echo=TRUE, eval=TRUE}
svmFit <- train(averBinnedF ~ . , data = E.train,
                 method = "svmRadial",
                 trControl = fitControl,
                 tuneLength = 8,
                 metric = "ROC")
svmFit
```

```{r, echo=TRUE, eval=TRUE}
# Call predict on the fitted object using the test data set
E.test.svm.predict <- predict(svmFit, newdata = E.test, na.action = na.omit, verbose = TRUE)
print(summary(as.data.frame(E.test.svm.predict)))
print(summary(as.data.frame(E.test$averBinnedF)))

# Assess confusion matrix
cm.svm.fit.E.test <- confusionMatrix(E.test.svm.predict, E.test$averBinnedF)
print(cm.svm.fit.E.test)
```

--> 



