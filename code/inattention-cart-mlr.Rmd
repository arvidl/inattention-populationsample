---
title: "Prediction of academic achievement in adolescents from teacher reports of inattention in childhood - a pattern classification study" 
subtitle: "AJ Lundervold, T Bø, A Lundervold"
output: html_notebook
---

```{r, echo=TRUE, eval=FALSE}
~GitHub/inattention-population/code/inattention-cart-mlr.Rmd
```

<small>
This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 
Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 
Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.
When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file).
</small>


<small>Organization of the data and the analysis:</small>

<img src="../images/Data_to_classes_pptx.jpg" width="500px" height="500px" />

### Data preparation

Builds on processing in *~GitHub/inattention-population/code/inattention-data-prep.Rmd*

```{r, echo=TRUE, eval=FALSE}
# In ~GitHub/inattention-population/code/inattention-populationsample-data-prep.Rmd we set:
# D$ave <- as.numeric(D$ave)
# D$snap1 <- mapvalues(as.factor(D$snap1), from = c("Not true","Somewhat true","Certainly true"), to = c("0","1","2"))
# # ...
# # Dataset for classification based on D3 and discretized average academic achievemnt
# C <- D3
# C$averBinned <- cut(aver, cutpoints, right=FALSE, include.lowest=TRUE,
#                      labels=c("L","M","H"))
# C <- subset(C, select = -c(ave))
# write.csv(C, file = "../data/inattention_nomiss_2397x12_snap_is_0_1_2_outcome_is_L_M_H.csv",row.names=FALSE)
```

```{r, echo=TRUE, eval=TRUE}
D3 <- read.csv(file = "../data/inattention_nomiss_2397x12_snap_is_0_1_2.csv") 
C <- read.csv(file = "../data/inattention_nomiss_2397x12_snap_is_0_1_2_outcome_is_L_M_H.csv")
D <- read.csv(file = "../data/inattention_nomiss_2397x12_snap_is_N_S_C_outcome_is_L_M_H.csv")
E <- read.csv(file = "../data/inattention_nomiss_2397x12_snap_is_0_1_2_outcome_is_0_1_2.csv")
str(C)
head(C)
```

```{r, echo=TRUE, eval=TRUE}
# Dataset for classification 
#library(Hmisc)
#describe(C)
library(pastecs)
tab <- stat.desc(C)
knitr::kable(tab, digits = 2, caption = 'Summary C')
```


## CART

From https://www.r-bloggers.com/a-brief-tour-of-the-trees-and-forests:

Tree methods such as CART (classification and regression trees) can be used as alternatives to logistic regression. It is a way that can be used to show the probability of being in any hierarchical group. The following is a compilation of many of the key R packages that cover trees and forests.  The goal here is to simply give some brief examples on a few approaches on growing trees and, in particular, the visualization of the trees. These packages include classification and regression trees, graphing and visualization, ensemble learning using random forests, as well as evolutionary learning trees. There are a wide array of package in R that handle decision trees including trees for longitudinal studies.  I have found that when using several combinations of these packages simultaneously that some of the function begin to fail to work.

## Conditional Inference Tree for SNAP inattention data (categorical predictor variables)


### party:

*party*: A Laboratory for Recursive Partytioning

A computational toolbox for recursive partitioning. The core of the package is ctree(), an implementation of conditional inference trees which embed tree-structured regression models into a well defined theory of conditional inference procedures. This non-parametric class of regression trees is applicable to all kinds of regression problems, including nominal, ordinal, numeric, censored as well as multivariate response variables and arbitrary measurement scales of the covariates. Based on conditional inference trees, cforest() provides an implementation of Breiman's random forests. The function mob() implements an algorithm for recursive partitioning based on parametric models (e.g. linear models, GLMs or survival regression) employing parameter instability tests for split selection. Extensible functionality for visualizing tree-structured regression models is available.


```{r, echo=TRUE, eval=TRUE}
library(party)
# Classification
frmla = averBinned ~ gender + grade + snap1 + snap2 + snap3 + 
    snap4 + snap5 + snap6 + snap7 + snap8 + snap9
fit.cla <- ctree(frmla, data=C)
print(fit.cla)
plot(fit.cla)
class(fit.cla)  # class type 
# Table of prediction errors
table(predict(fit.cla), C$averBinned)
# Estimated class probabilities
tr.pred = predict(fit.cla, newdata=C, type="prob")
print(head(tr.pred))
```

```{r, echo=TRUE, eval=TRUE}
n = nrow(C)
txt = sprintf("Conditional Inference Tree (ctree) for categorical SNAP inattention variables (n=%d)", n) 
plot(fit.cla, main=txt)
```

```{r, echo=TRUE, eval=TRUE}
# The class predictions of the tree for the learning sample (resubstitution)
summary(C$averBinned)
summary(predict(fit.cla))
``` 


### partykit:

The partykit package provides a flexible toolkit with infrastructure for learning, representing, summarizing, and visualizing a wide range of tree-structured regression and classification models. The functionality encompasses: (a) Basic infrastructure for repre- senting trees (inferred by any algorithm) so that unified print/plot/predict methods are available. (b) Dedicated methods for trees with constant fits in the leaves (or terminal nodes) along with suitable coercion functions to create such tree models (e.g., by rpart, RWeka, PMML). (c) A reimplementation of conditional inference trees (ctree, originally provided in the party package). (d) An extended reimplementation of model-based recur- sive partitioning (mob, also originally in party) along with dedicated methods for trees with parametric models in the leaves. This vignette gives a brief overview of the package and discusses in detail the generic infrastructure for representing trees (a). Items (b)–(d) are discussed in the remaining vignettes in the package.

Function partykit::ctree is a reimplementation of (most of) party::ctree employing the new party infrastructure of the partykit infrastructure. Although the new code was already extensively tested, it is not yet as mature as the old code. If you notice differences in the structure/predictions of the resulting trees, please contact the package maintainers. See also vignette("ctree", package = "partykit") for some remarks about the internals of the different implementations.

Conditional inference trees estimate a regression relationship by binary recursive partitioning in a conditional inference framework. Roughly, the algorithm works as follows: 1) Test the global null hypothesis of independence between any of the input variables and the response (which may be multivariate as well). Stop if this hypothesis cannot be rejected. Otherwise select the input variable with strongest association to the response. This association is measured by a p-value corresponding to a test for the partial null hypothesis of a single input variable and the response. 2) Implement a binary split in the selected input variable. 3) Recursively repeate steps 1) and 2).

The implementation utilizes a unified framework for conditional inference, or permutation tests, developed by Strasser and Weber (1999). The stop criterion in step 1) is either based on multiplicity adjusted p-values (testtype = "Bonferroni" in ctree_control) or on the univariate p-values (testtype = "Univariate"). In both cases, the criterion is maximized, i.e., 1 - p-value is used. A split is implemented when the criterion exceeds the value given by mincriterion as specified in ctree_control. For example, when mincriterion = 0.95, the p-value must be smaller than $0.05$ in order to split this node. This statistical approach ensures that the right-sized tree is grown without additional (post-)pruning or cross-validation. The level of mincriterion can either be specified to be appropriate for the size of the data set (and 0.95 is typically appropriate for small to moderately-sized data sets) or could potentially be treated like a hyperparameter (see Section~3.4 in Hothorn, Hornik and Zeileis, 2006). The selection of the input variable to split in is based on the univariate p-values avoiding a variable selection bias towards input variables with many possible cutpoints. The test statistics in each of the nodes can be extracted with the sctest method. (Note that the generic is in the strucchange package so this either needs to be loaded or sctest.constparty has to be called directly.) In cases where splitting stops due to the sample size (e.g., minsplit or minbucket etc.), the test results may be empty.

Predictions can be computed using predict, which returns predicted means, predicted classes or median predicted survival times and more information about the conditional distribution of the response, i.e., class probabilities or predicted Kaplan-Meier curves. For observations with zero weights, predictions are computed from the fitted tree when newdata = NULL.

By default, the scores for each ordinal factor x are 1:length(x), this may be changed for variables in the formula using scores = list(x = c(1, 5, 6)), for example.

For a general description of the methodology see Hothorn, Hornik and Zeileis (2006) and Hothorn, Hornik, van de Wiel and Zeileis (2006).

Hothorn T, Hornik K, Zeileis A (2006). Unbiased Recursive Partitioning: A Conditional Inference Framework. Journal of Computational and Graphical Statistics, 15(3), 651–674.

Hothorn T, Zeileis A (2015). partykit: A Modular Toolkit for Recursive Partytioning in R. Journal of Machine Learning Research, 16, 3905–3909.
http://www.jmlr.org/papers/v16/hothorn15a.html

*party* - A class for representing decision trees and corresponding accessor functions.

Objects of class party basically consist of a partynode object representing the tree structure in a recursive way and data. The data argument takes a data.frame which, however, might have zero columns. Optionally, a data.frame with at least one variable (fitted) containing the terminal node numbers of data used for fitting the tree may be specified along with a terms object or any additional (currently unstructured) information as info. Argument names defines names for all nodes in node.

Method names can be used to extract or alter names for nodes. Function node_party returns the node element of a party object. Further methods for party objects are documented in party-methods and party-predict. Trees of various flavors can be coerced to party, see party-coercion.

Two classes inherit from class party and impose additional assumptions on the structure of this object: Class constparty requires that the fitted slot contains a partitioning of the learning sample as a factor ("fitted") and the response values of all observations in the learning sample as ("response"). This structure is most flexible and allows for graphical display of the response values in terminal nodes as well as for computing predictions based on arbitrary summary statistics.

Class simpleparty assumes that certain pre-computed information about the distribution of the response variable is contained in the info slot nodes. At the moment, no formal class is used to describe this information.

*partynode* - A class for representing inner and terminal nodes in trees and functions for data partitioning.

A node represents both inner and terminal nodes in a tree structure. Each node has a unique identifier id. A node consisting only of such an identifier (and possibly additional information in info) is a terminal node.

Inner nodes consist of a primary split (an object of class partysplit) and at least two kids (daughter nodes). Kid nodes are objects of class partynode itself, so the tree structure is defined recursively. In addition, a list of partysplit objects offering surrogate splits can be supplied. Like partysplit objects, partynode objects aren't connected to the actual data.

Function kidids_node() determines how the observations in data[obs,] are partitioned into the kid nodes and returns the number of the list element in list kids each observations belongs to (and not it's identifier). This is done by evaluating split (and possibly all surrogate splits) on data using kidids_split.

Function fitted_node() performs all splits recursively and returns the identifier id of the terminal node each observation in data[obs,] belongs to. Arguments vmatch, obs and perm are passed to kidids_split.

Function formatinfo_node() extracts the the info from node and formats it to a character vector using the following strategy: If is.null(info), the default is returned. Otherwise, FUN is applied for formatting. The default function uses as.character for atomic objects and applies capture.output to print(info) for other objects. Optionally, a prefix can be added to the computed character string.

All other functions are accessor functions for extracting information from objects of class partynode.


```{r, echo=TRUE, eval=TRUE}
library(partykit) 
txt = sprintf("Conditional Inference Tree (ctree C in partykit) for categorical SNAP inattention variables (n=%d)", n) 
frmla = averBinned ~ gender + grade + snap1 + snap2 + snap3 + 
    snap4 + snap5 + snap6 + snap7 + snap8 + snap9
fit.pcla <- ctree(frmla, data=C)
class(fit.pcla)  # different class from before
print(fit.pcla)

# Table of prediction errors
table(predict(fit.pcla), C$averBinned)

# Estimated class probabilities
tr.pred = predict(fit.pcla, newdata=C, type="prob")
print(head(tr.pred))
# str(fit.pcla[2])
```

```{r, echo=TRUE, eval=TRUE}
plot(fit.pcla, gp = gpar(fontsize = 6),     # font size changed to 6, or 8
  inner_panel=node_inner,
  ip_args=list(
       abbreviate = FALSE, 
       id = TRUE),
  main=txt
  )
```

```{r, echo=TRUE, eval=TRUE}
pdf("../manuscript/Figs/cart_partykit_ctree_C.pdf", width = 8, height = 5)
plot(fit.pcla, gp = gpar(fontsize = 6),     # font size changed to 6, or 8
  inner_panel=node_inner,
  ip_args=list(
       abbreviate = FALSE, 
       id = TRUE),
  main=txt
  )
dev.off()
```



### rpart:

This package includes several example sets of data that can be used for recursive partitioning and regression trees.  Categorical or continuous variables can be used depending on whether one wants classification trees or regression trees. This package as well at the tree package are probably the two go-to packages for trees.  However, care should be taken as the tree package and the rpart package can produce very different results.
See also: https://gormanalysis.com/decision-trees-in-r-using-rpart/

Various parameters that control aspects of the rpart fit:
rpart.control(minsplit = 20, minbucket = round(minsplit/3), cp = 0.01, 
              maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, xval = 10,
              surrogatestyle = 0, maxdepth = 30, ...)

xval - number of cross-validations.

```{r, echo=TRUE, eval=TRUE}
library(rpart)

C <- read.csv(file = "../data/inattention_nomiss_2397x12_snap_is_0_1_2_outcome_is_L_M_H.csv")
D <- read.csv(file = "../data/inattention_nomiss_2397x12_snap_is_N_S_C_outcome_is_L_M_H.csv")

frmla = averBinned ~ gender + grade + snap1 + snap2 + snap3 + 
    snap4 + snap5 + snap6 + snap7 + snap8 + snap9

fitC = rpart(frmla, method="class", data=C)
fitD = rpart(frmla, method="class", data=D)

printcp(fitC) # display the results
plotcp(fitC) # visualize cross-validation results
summary(fitC) # detailed summary of splits
# plot tree
plot(fitC, uniform=TRUE, 
     main=list("Classification Tree (C) for SNAP, Gender, and Grade\n", 
               cex = 1.0, col = "black", font = 2))
text(fitC, use.n=TRUE, all=TRUE, cex=.6)
# tabulate some of the data
table(subset(C, grade=='3rd')$averBinned)
```
```{r, echo=TRUE, eval=TRUE}
library(rattle)
library(rpart.plot)
library(RColorBrewer)
 
fancyRpartPlot(fitC)
```

```{r, echo=TRUE, eval=TRUE}
library(rattle)
library(rpart.plot)
library(RColorBrewer)
 
fancyRpartPlot(fitD)
```

#### tree:

This is the primary R package for classification and regression trees.  It has functions to prune the tree as well as general plotting functions and the mis-classifications (total loss). The output from tree can be easier to compare to the General Linear Model (GLM) and General Additive Model (GAM) alternatives.

```{r, echo=TRUE, eval=TRUE}
library(tree)
tr = tree(averBinned ~ ., data=C)
summary(tr)
plot(tr)
text(tr)
```

#### maptree:

Graph a classification or regression tree with a hierarchical tree diagram, optionally including colored symbols at leaves and additional info at intermediate nodes.

```{r, echo=TRUE, eval=TRUE}
library(maptree)
library(cluster)
library(rpart)
draw.tree(clip.rpart(rpart(C), best=7), nodeinfo=TRUE, units="",cases="obs", digits=0)
```

## Cross-validating a CART model

library(rpart)

library(cvTools)

data(iris)

cvFit(rpart, formula=Species~., data=iris,
      cost=function(y, yHat) (y != yHat) + 0, predictArgs=c(type='class'))
      


### mlr: Machine Learning in R
https://www.rdocumentation.org/packages/mlr/versions/2.9?

```{r, echo=TRUE, eval=TRUE}
# http://stackoverflow.com/questions/36802846/how-to-tune-multiple-parameters-using-caret-package
# http://mlr-org.github.io/mlr-tutorial/release/html/measures/index.html

library(mlr)
library(psych)

# Read and check the data
C <- read.csv(file = "../data/inattention_nomiss_2397x12_snap_is_0_1_2_outcome_is_L_M_H.csv")
str(C)
headTail(C)

# Create a classification, regression, survival, cluster, cost-sensitive classification or multilabel task
# https://rdrr.io/cran/mlr/man/Task.html
# id - Id string for object. Default is the name of the R variable passed to data.
# data - A data frame containing the features and target variable(s).
# target - Name(s) of the target variable(s).
rpart.task = classif.task = makeClassifTask(id = "inattention", data = C, target = "averBinned")


# Create a description object for a resampling strategy
# https://rdrr.io/cran/mlr/man/makeResampleDesc.html
# method - “CV” for cross-validation, “LOO” for leave-one-out, “RepCV” for repeated cross-validation, 
# “Bootstrap” for out-of-bag bootstrap, “Subsample” for subsampling, “Holdout” for holdout.
# predict -  What to predict during resampling: “train”, “test” or “both” sets. Default is “test”.
# iters - Number of iterations, for “CV”, “Subsample” and “Boostrap”.
# reps - Repeats for “RepCV”. Here iters = folds * reps. Default is 10.
# folds - Folds in the repeated CV for RepCV. Here iters = folds * reps. Default is 10.
# stratify - Should stratification be done for the target variable? For classification tasks, 
#   this means that the resampling strategy is applied to all classes individually and the resulting 
#   index sets are joined to make sure that the proportion of observations in each training 
#   set is as in the original data set. Useful for imbalanced class sizes.
# We will use 10-fold cross-validation with 10 repetitions to assess the quality of a 
# specific parameter setting. 
resamp = makeResampleDesc("RepCV", predic = "test", folds = 10, reps = 10, stratify = TRUE)

# Create learner object
# https://rdrr.io/cran/mlr/man/makeLearner.html
# cl - Class of learner. By convention, all classification learners start with “classif.”, 
#   all regression learners with “regr.”, all survival learners start with “surv.”, 
#   all clustering learners with “cluster.”, and all multilabel classification learners start with
#   “multilabel.”. A list of all integrated learners is available on the learners help page.
# predict.type - Classification: “response” (= labels) or “prob” (= probabilities and labels by 
#   selecting the ones with maximal probability). Regression: “response” (= mean response) or 
#   “se” (= standard errors and mean response). Survival: “response” (= some sort of orderable risk) 
#   or “prob” (= time dependent probabilities). Clustering: “response” (= cluster IDS) or 
#   “prob” (= fuzzy cluster membership probabilities), Multilabel: “response” (= logical matrix 
#   indicating the predicted class labels) or “prob” (= probabilities and corresponding 
#   logical matrix indicating class labels). Default is “response”.
lrn = makeLearner("classif.rpart", predict.type = "response" )

# Get a description of all possible parameter settings for a learner.
# https://rdrr.io/cran/mlr/man/getParamSet.html
# https://mlr-org.github.io/mlr-tutorial/release/html/learner/index.html
# You can also check all the tunable params. The Learner object is a list and the following 
# elements contain information regarding the hyperparameters and the type of prediction.
getParamSet(lrn)

# Nested Resampling
# https://mlr-org.github.io/mlr-tutorial/release/html/nested_resampling/index.html
makeFilterWrapper(lrn)


# Training a Learner
# Training a learner means fitting a model to a given data set. 
# In mlr this can be done by calling function train on a Learner and a suitable Task.
# Training a learner works the same way for every type of learning problem.
# Function train returns an object of class WrappedModel, which encapsulates the fitted model, 
# i.e., the output of the underlying R learning method.
# Additionally, it contains some information about the Learner, the Task, the features and 
# observations used for training, and the training time. 
# A WrappedModel can subsequently be used to make a prediction for new observations.
mod = train(lrn, rpart.task)

## Peak into mod
names(mod)
mod$learner

# Extract the fitted model ( mod$learner.model)
getLearnerModel(mod)

# Plot the fitted model
plot(getLearnerModel(mod))

pred = predict(mod, newdata = C)
head(as.data.frame(pred))

# In order to get predicted posterior probabilities we have to create a Learner 
# with the appropriate predict.type.  
lrn_prob = makeLearner("classif.rpart", predict.type = "prob")
mod_prob = train(lrn_prob, rpart.task)
pred_prob = predict(mod_prob, newdata = C)
head(as.data.frame(pred_prob))

# In addition to the probabilities, class labels are predicted by choosing the class with 
# the maximum probability and breaking ties at random.
# The predicted posterior probabilities can be accessed via the getPredictionProbabilities function.
head(getPredictionProbabilities(pred_prob))

# Confusion matrix
# https://www.rdocumentation.org/packages/mlr/versions/2.9/topics/getConfMatrix
# Calculates confusion matrix for (possibly resampled) prediction. 
# Rows indicate true classes, columns predicted classes. The marginal elements count the 
# number of classification errors for the respective row or column, i.e., the number of 
# errors when you condition on the corresponding true (rows) or predicted (columns) class. 
# The last element in the margin diagonal displays the total amount of errors. 
# Note that for resampling no further aggregation is currently performed. 
# All predictions on all test sets are joined to a vector yhat, as are all labels 
# joined to a vector y. Then yhat is simply tabulated vs y, as if both were computed 
# on a single test set. This probably mainly makes sense when cross-validation is used for resampling.
print(getConfMatrix(pred, relative = FALSE))
print(getConfMatrix(pred, relative = TRUE))

# Feature Selection
# Often, data sets include a large number of features. The technique of extracting a subset of 
# relevant features is called feature selection. Feature selection can enhance the 
# interpretability of the model, speed up the learning process and improve the learner performance. 
# There exist different approaches to identify the relevant features. mlr supports filter 
# and wrapper methods.

# Calculates feature filter values.
# https://rdrr.io/cran/mlr/man/generateFilterValuesData.html
# Calculates numerical filter values for features. For a list of features, use listFilterMethods.
# fv = generateFilterValuesData(rpart.task, method = "cforest.importance")
fv = generateFilterValuesData(rpart.task, method = "rf.importance")
print(fv)
print(fv$data)
# plotFilterValuesGGVIS(fv)
plotFilterValues(fv)
getFilteredFeatures(fv)

# Create control structures for tuning
# https://rdrr.io/cran/mlr/man/TuneControl.html
# makeTuneControlGrid - Grid search. All kinds of parameter types can be handled. 
#   You can either use their correct param type and resolution, or discretize them yourself by 
#   always using makeDiscreteParam in the par.set passed to tuneParams.
# same.resampling.instance - Should the same resampling instance be used for all evaluations to 
#   reduce variance? Default is TRUE.
# resolution - Resolution of the grid for each numeric/integer parameter in par.set. 
#   For vector parameters, it is the resolution per dimension. Either pass one resolution for all 
#   parameters, or a named vector. You can pass resolution = N if you want the algorithm to 
#   select N tune params given upper and lower bounds to a NumericParam
#   instead of a discrete one. Default is 10.
control.grid = makeTuneControlGrid(same.resampling.instance = TRUE, resolution = 10) 

# Tuning Hyperparameters
# https://mlr-org.github.io/mlr-tutorial/devel/html/tune/index.html
# Many machine learning algorithms have hyperparameters that need to be set. 
# If selected by the user they can be specified as explained on the tutorial page on Learners 
# -- simply pass them to makeLearner. Often suitable parameter values are not obvious and it is 
# preferable to tune the hyperparameters, that is automatically identify values that lead to 
# the best performance.
# We first must define a space to search when tuning our learner. 
# For example, maybe we want to tune several specific values of a hyperparameter or perhaps we 
# want to define a space from 10^{−10} to 10^{10} and let the optimization algorithm decide 
# which points to choose. In order to define a search space, we create a ParamSet object, 
# which describes the parameter space we wish to search. This is done via the function makeParamSet.
# https://www.rdocumentation.org/packages/ParamHelpers/versions/1.1-36/topics/makeParamSet?
# Multiple sets can be concatenated with c
# Create A Description Object For A Parameter, e.g.
# makeDiscreteParam(id, values, requires = NULL)
# id - Name of parameter
# values - Possible discrete values. Instead of using a vector of atomic values, you are also 
#   allowed to pass a list of quite complex R objects, which are used as discrete choices. 
# In rpart:
# minsplit - the minimum number of observations that must exist in a node in order for a split to be attempted
# cp - complexity parameter. Any split that does not decrease the overall lack of fit by a factor 
#   of cp is not attempted. For instance, with anova splitting, this means that the overall R-squared 
#   must increase by cp at each step. The main role of this parameter is to save computing time by 
#   pruning off splits that are obviously not worthwhile. Essentially,the user informs the program 
#   that any split which does not improve the fit by cp will likely be pruned off by cross-validation, 
#   and that hence the program need not pursue it.
ps = makeParamSet(
  makeDiscreteParam("cp", values = seq(0,0.2,0.1)),
  makeDiscreteParam("minsplit", values = c(30,40,50))
)
print(ps)

# Hyperparameter Tuning.
# https://www.rdocumentation.org/packages/mlr/versions/2.9/topics/tuneParams?
# Optimizes the hyperparameters of a learner. Allows for different optimization methods, 
# such as grid search, evolutionary strategies, iterated F-race, etc. 
# You can select such an algorithm (and its settings) by passing a corresponding control object. 
# tuneParams simply performs the cross-validation for every element of the cross-product and 
# selects the parameter setting with the best mean performance. 
# If no performance measure is specified, by default the error rate (mmce) is used.
# You can pass other measures and also a list of measures to tuneParams. 
# In the latter case the first measure is optimized during tuning, the others are simply evaluated. 
# If you are interested in optimizing several measures simultaneously have a look at Advanced Tuning.
# The actual tuning, with accuracy as evaluation metric
# tuneParams(learner, task, resampling, measures, par.set, control, show.info = getMlrOption("show.info"))
# learner - The learner. If you pass a string the learner will be created via makeLearner
# task - The task
# resampling - Resampling strategy to evaluate points in hyperparameter space. 
#   If you pass a description, it is instantiated once at the beginning by default, 
#   so all points are evaluated on the same training/test sets. If you want to change that behavior, 
#   look at TuneControl
# measures - Performance measures to evaluate. The first measure, aggregated by the first 
#   aggregation function is optimized, others are simply evaluated. 
#   Default is the default measure for the task, see getDefaultMeasure.
# par.set - Collection of parameters and their constraints for optimization. 
#   Dependent parameters with a requires field must use quote and not expression to define it.
# control -  Control object for search method. Also selects the optimization algorithm for tuning.
# show.info -  Print verbose output on console? Default is set via configureMlr
res = tuneParams(lrn, task = rpart.task, resampling = resamp, measures = list(acc,timetrain),
                 par.set = ps, control = control.grid, show.info = TRUE)

print(res)
print(as.data.frame(res$opt.path))

# https://mlr-org.github.io/mlr-tutorial/devel/html/hyperpar_tuning_effects/index.html
names(res)
print(c(res$x$cp, res$x$minsplit))
print(res$y)

# Investigating hyperparameter tuning effects
# https://www.rdocumentation.org/packages/mlr/versions/2.9/topics/generateHyperParsEffectData?
generateHyperParsEffectData(res, include.diagnostics = TRUE)
```



<!-- regular html comment 


--> 



